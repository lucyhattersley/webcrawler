from robot import Page, Site

page = Page('http://www.nytimes.com')

page.get_domain()

# # Pop link from site
# page = site.pop_links()

# # get soup
# soup = page.get_soup()

# # get page links from soup
# links = page.get_links()

# # while links
# while len(links) > 0
# link = links.pop()

#     # is link valid
#     if page.is_valid(link) AND page.is_internal(link)
#         if link is not complete
#             page.make_complete(link)
#         if link not in all_links
#             all_links.append(link)
#             add_to_database(link)
#         else:
#             update_database(link)




