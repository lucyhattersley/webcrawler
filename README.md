Webcrawler scans webpages and extracts links. It then uses these to find other (internal) pages of a website and saves all links it finds to a text file. It is used to create a list of all the webpages in a website. Further iterations will also collect information on each page, such as number of outbound links, inbound links, title, keywords etc. It is hoped that eventually it will be smart enough to gather detailed information about which are the most important internal pages in a website.

For now it just scans a website and creates a list of webpages. 

Webcrawler uses BeautifulSoup 4. Install with "pip install beautifulsoup4" first.

lucy.a.hattersley@gmail.com